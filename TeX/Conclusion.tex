\chapter{Conclusion and future developments}
\label{chap:CONCLUSION}




In conclusion, the development of the algorithm has given successful results overall, which will be summarized in the following Section. Moreover, several ideas to improve the algorithm are proposed in Section \ref{sec:FUTURE_DEVELOPMENTS}. 

\section{Overview of the results}
\label{sec:OVERVIEW}

\paragraph*{Weight clipping optimization}
The search for the optimal $W$ has showed that for a certain value of the parameter the $t_\mathrm{obs}$ distribution converges after training at a $\chi^2$ distribution with a number of dof equals to the free parameters of the neural network. By the repetition of this procedure with different reference size, a trend for the optimal $W$ was found and from this curve it is possible to predict an approximate value for the optimal $W$ in a different condition. Last but not least, it was found that the value of $W$ tends to saturate when the reference size is much populated respect to the size of the dataset of the observed data. Therefore, this phase of training can be avoided when the reference size is wide enough, in fact a good choice of the weight clipping could be the asymptotic value of $W$.

\paragraph*{Training with signal events}
The analysis of the results has showed that the network is capable of distinguishing new physics signal events from the backgound ones, with a performance depending on the dataset employed. The proof of this result is given by the median significance obtained from the training with signal events. In the case of \textbf{Zmumu-Zprime} dataset, the performance of the algorithm is higher and it is greater than $3\sigma$, which means that the network can discern clearly between signal and background events. The ideal significance is still far, as its value is $\sim 11\sigma$, however it is calculated with a complete knowledge on the true data distribution, while in the training process the network has no knowledge on these informations.

A more delicate discussion has to be done with the results coming from the \textbf{EFT\_YW06} dataset. In this case, the median significance obtained after training is lower and it doesn't exceed the threshold of $3\sigma$ and more improvements should be done on the algorithm. However, the shape of the $t_\mathrm{obs}$ distribution with signal events is different from the shape of the distribution with only background, obtained in the optimization phase of the parameter $W$. Therefore, the network is still capable of distinguish signal from background, but with a worse performance.

Lastly, the plot of the output of the network $f(x)$, when plotted versus the invariant mass $M_{Z}$ corresponding to the event $x$, confirms that the trained network has learnt to discern the resonant peaks in the mass distribution, although $M_{Z}$ is a highly not-linear combination of the features $p_{T,1}$, $p_{T,2}$, $\eta_{1}$, $\eta_{2}$ and $\Delta \phi$. This is a successful result as the invariant mass was not given in input to the network.





\section{Future developments of the algorithm}
\label{sec:FUTURE_DEVELOPMENTS}

\paragraph*{New input features}
In order to achieve a higher significance through the trained network, a possible solution is add other dimentions in the input space, which means to add new features in addition to the HLFs previously listed. There are several choices in this sense, but one of the most suited seems to be the invariant mass $M_{Z}$. This feature contains a very high level information and it can boost significantly the classification power of the network. However, more specific tests are needed in order to give quantitative answers and by adding another feature the time needed for the algorithm routine becomes larger. In facr, the number of parameters to optimize becomes more if the nuber of layers of the network stays the same.

\paragraph*{Distributed computing and parallelization}
Another important development can be made in this sense. First of all, a way to reduce the time needed for sampling $t_\mathrm{obs}$ distributions is exploiting a bigger cluster of machines and with more resources for every single worker. An alternative to this possible solution could be a cluster of GPUs. In fact, these units have an architecture which make them suitable to this kind of tasks. Moreover, Keras API employed for this work is optimized for GPU distributed computing, so it spreads the various computations done during the training phase to the graphic unit, if there is one.

