\chapter{Conclusion and future developments}
\label{chap:CONCLUSION}




In conclusion, the development of the algorithm has given successful results overall and they will be summarized in the following Section. Moreover, several ideas to improve the algorithm are proposed in Section \ref{sec:FUTURE_DEVELOPMENTS}. 

\section{Overview of the results}
\label{sec:OVERVIEW}

\paragraph*{Weight clipping optimization}
The search for the optimal $W$ has showed that for a certain value of the parameter the $t_\mathrm{obs}$ distribution converges after training at a $\chi^2$ distribution with a number of dofs equal to the number of free parameters of the neural network. By the repetition of this procedure with different reference sizes, a trend for the optimal $W$ was found and from this curve it is possible to predict an approximate value for the optimal $W$ in a different condition. Last but not least, it was found that the value of $W$ tends to saturate when the reference size is much populated respect to the size of the dataset of the observed data. Therefore, this phase of training can be avoided when the reference size is wide enough, in fact a good choice of the weight clipping could be the asymptotic value of $W$.

\paragraph*{Training with signal events}
The analysis of the results has showed that the network is capable of distinguishing new physics signal events from the backgound ones, with a performance depending on the dataset employed. The proof of this result is given by the median significance obtained from the training with signal events. In the case of \textbf{Zmumu-Zprime} dataset, the performance of the algorithm is higher and it is greater than $3\sigma$, which means that the network can discern clearly between signal and background events. The ideal significance is still far, as its value is $\sim 11\sigma$, however it is calculated with a complete knowledge on the true data distribution, while in the training process the network has no knowledge on these informations.

A more delicate discussion has to be done with the results coming from the \textbf{EFT\_YW06} dataset. In this case, the median significance obtained after training is lower and it doesn't exceed the threshold of $3\sigma$ and more improvements should be done on the algorithm. However, the shape of the $t_\mathrm{obs}$ distribution with signal events is different from the shape of the distribution with only background, obtained in the optimization phase of the parameter $W$. Therefore, the network is still capable of distinguishing signal from background, but with a worse performance.

Lastly, the plot of the output of the network $f(x)$, when plotted versus the invariant mass $M_{Z}$ corresponding to the event $x$, confirms that the trained network has learnt to discern the resonant peaks in the mass distribution, although $M_{Z}$ is a highly non-linear combination of the features $p_{T,1}$, $p_{T,2}$, $\eta_{1}$, $\eta_{2}$ and $\Delta \phi$. This is a successful result as the invariant mass was not given to the input neurons of the network.





\section{Future developments of the algorithm}
\label{sec:FUTURE_DEVELOPMENTS}

\paragraph*{New input features}
In order to achieve a higher significance through the trained network, a possible solution can be adding other dimentions to the input space, which means to add new features in addition to the HLFs previously listed. There are several choices in this sense, but one of the most suited seems to be the invariant mass $M_{Z}$. This feature contains a very high level information and it can boost significantly the classification power of the network. However, more specific tests are needed in order to give quantitative answers and by adding another feature the time needed for the algorithm routine becomes larger. In fact, the number of parameters to optimize becomes larger if the nuber of layers of the network stays the same.

\paragraph*{Distributed computing and parallelization}
Another important development can be made in this sense. First of all, a way to reduce the time needed for sampling the $t_\mathrm{obs}$ distribution is exploiting a bigger cluster of machines and with more resources for every single worker. An alternative to this possible solution could be a cluster of GPUs. In fact, these units have an architecture which make them suitable to this kind of tasks. Moreover, Keras API employed for this work is optimized for GPU distributed computing, so it spreads the various computations done during the training phase to the graphic unit, if there is one.

Last but not least, the most remarkable improvement that can be done to the procedure is the parallelization of a single process. In order to explain the key idea that makes this technique theoretically possible, we have to split the ensemble of reference and data into $n$ subsets $(x^{(k)},y^{(k)})$, with $k=1,\dots,n$, and then we have to rewrite the loss function into $n$ pieces, computed over the $n$ subsets:

\begin{align}
	L[f]
	&= 
	\sum_{(x,y)} \left[
	(1-y) \frac{N(\R)}{N_{\mathcal{R}}} (e^{f(x)} - 1) - yf(x)
	\right]	\nonumber \\
	&=
	\sum_{k=1}^{n} \sum_{(x^{(k)},y^{(k)})} \left[
	(1-y^{(k)}) \frac{N(\R)}{N_{\mathcal{R}}} (e^{f(x^{(k)})} - 1) - y^{(k)}f(x^{(k)})
	\right]	\label{eqn:PARALLEL_LOSS}	\\
	&=
	\sum_{k=1}^{n}
	L^{(k)}[f]	\label{eqn:JOINT_LOSS}
\end{align}

\noindent
Equations \ref{eqn:PARALLEL_LOSS} and \ref{eqn:JOINT_LOSS} show that the algorithm is scalable on $n$ different workers. In fact, the $k^\text{th}$-worker can compute $L^{(k)}[f]$ in parallel with the other workers, with $k=1,\dots,n$, and then every worker communicates in order to compute $L[f]$ by summing every $L^{(k)}[f]$. By this way, the computation is $n$ times faster respect to the standard procedure, since the time of computation grows linearly with the size of the reference and the data. The same idea is applicable to the computation of the gradient, needed for parameters update. Concerning the fulfillment of this idea, there are several tools which can manage parallel computing. One of the most known is the framework \textsc{Apache Spark}, made for parallel computing and automatic learning tasks.

\bigskip
Putting all the previous ideas together is the actual prospective of this work. A boost in the performance and in the computation time is needed before applying this analysis to real datasets taken by the LHC detectors, in particular CMS, Their size is bigger than the sizes analyzed in this work. However, the results obtained are encouraging and there is extensive room for improvement.