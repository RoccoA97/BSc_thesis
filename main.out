\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{High-Energy Particle Physics and Standard Model}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{Researches for new physics}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Benefits of a model-independent approach}{section.1.1}% 4
\BOOKMARK [1][-]{section.1.2}{Machine Learning answer to the problem}{chapter.1}% 5
\BOOKMARK [0][-]{chapter.2}{Introduction to Neural Networks}{}% 6
\BOOKMARK [1][-]{section.2.1}{Artificial Neural Networks}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.2}{Choice of NN architecture and activation functions}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.2.1}{Number of hidden layers}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.2}{Number of neurons per hidden layer}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.3}{Activation functions}{section.2.2}% 11
\BOOKMARK [1][-]{section.2.3}{Loss functions}{chapter.2}% 12
\BOOKMARK [1][-]{section.2.4}{Backpropagation algorithm}{chapter.2}% 13
\BOOKMARK [1][-]{section.2.5}{Updating free parameters: Optimizers}{chapter.2}% 14
\BOOKMARK [2][-]{subsection.2.5.1}{An example of algorithm with fixed learning rate}{section.2.5}% 15
\BOOKMARK [2][-]{subsection.2.5.2}{An example of algorithm with adaptive learning rates}{section.2.5}% 16
\BOOKMARK [1][-]{section.2.6}{Other techniques}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.6.1}{Regularizers}{section.2.6}% 18
\BOOKMARK [2][-]{subsection.2.6.2}{Dropout}{section.2.6}% 19
\BOOKMARK [2][-]{subsection.2.6.3}{Parameter initialization strategies}{section.2.6}% 20
\BOOKMARK [0][-]{chapter.3}{CERN research for exotic particles through LHC collider}{}% 21
\BOOKMARK [1][-]{section.3.1}{An insight into LHC structure}{chapter.3}% 22
\BOOKMARK [1][-]{section.3.2}{CMS experiment}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.3}{From data to discoveries: resonance formation study}{chapter.3}% 24
\BOOKMARK [0][-]{chapter.4}{Z +- decaying process}{}% 25
\BOOKMARK [1][-]{section.4.1}{The Z0 boson}{chapter.4}% 26
\BOOKMARK [1][-]{section.4.2}{High Level Features for +- decay analysis}{chapter.4}% 27
\BOOKMARK [1][-]{section.4.3}{The datasets analyzed}{chapter.4}% 28
\BOOKMARK [0][-]{chapter.5}{Development of the algorithm for NP research}{}% 29
\BOOKMARK [1][-]{section.5.1}{Statistical foundations}{chapter.5}% 30
\BOOKMARK [2][-]{subsection.5.1.1}{Construction of a test statistic}{section.5.1}% 31
\BOOKMARK [2][-]{subsection.5.1.2}{Ideal test statistic}{section.5.1}% 32
\BOOKMARK [2][-]{subsection.5.1.3}{Adaptation of t\(D\) as a loss function}{section.5.1}% 33
\BOOKMARK [1][-]{section.5.2}{The algorithm}{chapter.5}% 34
\BOOKMARK [1][-]{section.5.3}{Computing resources}{chapter.5}% 35
\BOOKMARK [2][-]{subsection.5.3.1}{TensorFlow back-end and Keras API}{section.5.3}% 36
\BOOKMARK [2][-]{subsection.5.3.2}{LSF and clustering}{section.5.3}% 37
\BOOKMARK [1][-]{section.5.4}{Degrees of freedom of the network}{chapter.5}% 38
\BOOKMARK [1][-]{section.5.5}{The "look-elsewhere" effect}{chapter.5}% 39
\BOOKMARK [1][-]{section.5.6}{Weight clipping}{chapter.5}% 40
\BOOKMARK [0][-]{chapter.6}{Tuning of weight clipping parameter}{}% 41
\BOOKMARK [1][-]{section.6.1}{A criterion for optimal weight clipping}{chapter.6}% 42
\BOOKMARK [1][-]{section.6.2}{Results for optimal weight clipping search}{chapter.6}% 43
\BOOKMARK [0][-]{chapter.7}{Training with signal events and results}{}% 44
\BOOKMARK [1][-]{section.7.1}{Ideal significance computation}{chapter.7}% 45
\BOOKMARK [1][-]{section.7.2}{Training results: tobs distribution and observed significance}{chapter.7}% 46
\BOOKMARK [1][-]{section.7.3}{Training results: NN output analysis}{chapter.7}% 47
\BOOKMARK [0][-]{chapter.8}{Conclusion and future developments}{}% 48
