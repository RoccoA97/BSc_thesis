\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The progress of Particle Physics}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The research for new physics}{1}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}The need for a model-independent approach}{1}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Why do we choose Neural Networks?}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction to Neural Networks}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Artificial Neural Networks}{3}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Fully connected ANN.\relax }}{3}{figure.2.1}\protected@file@percent }
\newlabel{fig:FCNN}{{2.1}{3}{Fully connected ANN.\relax }{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Partially connected ANN.\relax }}{3}{figure.2.2}\protected@file@percent }
\newlabel{fig:NFCNN}{{2.2}{3}{Partially connected ANN.\relax }{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Choice of NN architecture and activation functions}{4}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Number of hidden layers}{4}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Number of neurons per hidden layer}{5}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Activation functions}{5}{subsection.2.2.3}\protected@file@percent }
\newlabel{fig:STEP}{{2.2.3}{5}{Activation functions}{subsection.2.2.3}{}}
\newlabel{fig:SIGMOID}{{2.2.3}{5}{Activation functions}{equation.2.2.5}{}}
\newlabel{fig:TANH}{{2.2.3}{6}{Activation functions}{equation.2.2.6}{}}
\newlabel{fig:RELU}{{2.2.3}{6}{Activation functions}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Loss functions}{6}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Backpropagation algorithm}{7}{section.2.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation through a typical deep neural network and the computation of the cost function. The loss $L(\mathaccentV {hat}05E{y},y)$ depends on the output $\mathaccentV {hat}05E{y}$ and on the target $y$. The symbol $\theta $ will be used to indicate both weights and biases.\relax }}{8}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{FORWARD_PROPAGATION}{{1}{8}{Forward propagation through a typical deep neural network and the computation of the cost function. The loss $L(\hat {y},y)$ depends on the output $\hat {y}$ and on the target $y$. The symbol $\theta $ will be used to indicate both weights and biases.\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backward computation for the deep neural network of Algorithm \ref  {FORWARD_PROPAGATION}, which uses, in addition to the input $x$, a target $y$. This computation yields the gradients on the activations $a^{(k)}$ for each layer $k$, starting from the output layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer's output should change to reduce error, one can obtain the gradient on the parameters of each layer.\relax }}{8}{algorithm.2}\protected@file@percent }
\newlabel{BACKWARD_COMPUTATION}{{2}{8}{Backward computation for the deep neural network of Algorithm \ref {FORWARD_PROPAGATION}, which uses, in addition to the input $x$, a target $y$. This computation yields the gradients on the activations $a^{(k)}$ for each layer $k$, starting from the output layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer's output should change to reduce error, one can obtain the gradient on the parameters of each layer.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Updating free parameters: Optimizers}{8}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}An example of algorithm with fixed learning rate}{9}{subsection.2.5.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent (SGD) update at training iteration $k$\relax }}{9}{algorithm.3}\protected@file@percent }
\newlabel{SGD}{{3}{9}{Stochastic Gradient Descent (SGD) update at training iteration $k$\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}An example of algorithm with adaptive learning rates}{9}{subsection.2.5.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Adam algorithm\relax }}{9}{algorithm.4}\protected@file@percent }
\newlabel{ADAM}{{4}{9}{Adam algorithm\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Advanced tecniques}{9}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Regularizers}{9}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Dropout}{10}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Parameter initialization strategies}{10}{subsection.2.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Xavier and He initializations.\relax }}{10}{table.caption.3}\protected@file@percent }
\newlabel{tab:XAVIER_HE_INITIALIZATIONS}{{2.1}{10}{Xavier and He initializations.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}CERN research for exotic particles through LHC collider}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}An insight into LHC structure}{11}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}CMS experiment}{12}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces CMS detector section.\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:CMS_SECTION}{{3.1}{12}{CMS detector section.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces CMS detector transverse section.\relax }}{13}{figure.caption.5}\protected@file@percent }
\newlabel{fig:CMS_TRANSVERSE_SECTION}{{3.2}{13}{CMS detector transverse section.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}From data to discoveries: resonance formation study}{14}{section.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Resonance plot.\relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{fig:RESONANCE_PLOT}{{3.3}{14}{Resonance plot.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}$Z \rightarrow \mu ^{+}\mu ^{-}$ decaying process}{17}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The $Z^{0}$ boson}{17}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}High Level Features for the analysis of the decay}{18}{section.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces High Level Features legend.\relax }}{18}{table.caption.7}\protected@file@percent }
\newlabel{tab:HLF}{{4.1}{18}{High Level Features legend.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The dataset analyzed}{18}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Data distribution.\relax }}{19}{figure.caption.8}\protected@file@percent }
\newlabel{fig:DATA_DISTRIBUTIONS}{{4.1}{19}{Data distribution.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Features and invariant mass distributions.\relax }}{19}{figure.caption.9}\protected@file@percent }
\newlabel{fig:FEATURES_DISTRIBUTIONS}{{4.2}{19}{Features and invariant mass distributions.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Features and invariant mass distributions with signal+background in the same dataset.\relax }}{20}{figure.caption.10}\protected@file@percent }
\newlabel{fig:SIG_PLUS_BKG_DISTRIBUTIONS}{{4.3}{20}{Features and invariant mass distributions with signal+background in the same dataset.\relax }{figure.caption.10}{}}
\citation{wulzer}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Development of the algorithm for NP research}{21}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Statistical foundations}{21}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Construction of a test statistic}{21}{subsection.5.1.1}\protected@file@percent }
\newlabel{eqn:T}{{5.2}{21}{Construction of a test statistic}{equation.5.1.2}{}}
\citation{wulzer}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Ideal test statistic}{22}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Adaptation of $t(\EuScript  {D})$ as a loss function}{22}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}The algorithm}{23}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Computing resources}{23}{section.5.3}\protected@file@percent }
\citation{python}
\citation{keras}
\citation{tensorflow}
\citation{cvmfs}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}\textsc  {TensorFlow} back-end and \textsc  {Keras} API}{24}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}LSF and clustering}{24}{subsection.5.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Available queues.\relax }}{25}{table.caption.11}\protected@file@percent }
\newlabel{tab:QUEUES}{{5.1}{25}{Available queues.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Machines and resources.\relax }}{25}{table.caption.12}\protected@file@percent }
\newlabel{tab:LONG_QUEUE_MACHINES}{{5.2}{25}{Machines and resources.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Degrees of freedom of the network}{25}{section.5.4}\protected@file@percent }
\citation{look-elsewhere}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Neural Network scheme to understand the number of degrees of freedom.\relax }}{26}{figure.caption.13}\protected@file@percent }
\newlabel{fig:DOF}{{5.1}{26}{Neural Network scheme to understand the number of degrees of freedom.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The "look-elsewhere" effect}{26}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Weight clipping}{26}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Tuning of weight clipping parameter}{29}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Sizes of the datasets for different tests.\relax }}{29}{table.caption.14}\protected@file@percent }
\newlabel{tab:W_CLIP_TEST_SIZES}{{6.1}{29}{Sizes of the datasets for different tests.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}A criterion for optimal weight clipping}{29}{section.6.1}\protected@file@percent }
\bibcite{python}{1}
\bibcite{keras}{2}
\bibcite{tensorflow}{3}
\bibcite{cvmfs}{4}
\bibcite{lsf}{5}
\bibcite{madgraph}{6}
\bibcite{deeplearningbook}{7}
\bibcite{oreilly}{8}
\bibcite{cern}{9}
\bibcite{cms}{10}
\bibcite{bettini}{11}
\bibcite{zboson}{12}
\bibcite{look-elsewhere}{13}
\bibcite{wulzer}{14}
\bibcite{baldi}{15}
