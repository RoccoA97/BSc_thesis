\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}High-Energy Particle Physics and Standard Model}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Standard Model of Physics.\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SM_PARTICLES}{{1.1}{1}{Standard Model of Physics.\relax }{figure.caption.3}{}}
\citation{cms_results}
\citation{cms_results}
\citation{cms_results}
\citation{cms_results}
\citation{baldi}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Researches for new physics}{2}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Benefits of a model-independent approach}{2}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Machine Learning answer to the problem}{2}{section.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Best exclusion limits for the masses of the mother particles, for RPC scenarios, for m(LSP) = $0~\si {GeV}$ (dark shades) and $m(\text  {mother}) - m(\text  {LSP}) = 200~\si {GeV}$ (light shades); for each topology, for all results. In this plot, the lowest mass range is $m(\text  {mother})=0$, but results are available starting from a certain mass depending on the analyses and topologies. Branching ratios of one are assumed, values shown in plot are to be interpreted as upper bounds on the mass limits \cite  {cms_results}.\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:MODEL_DEPENDENT_1}{{1.2}{3}{Best exclusion limits for the masses of the mother particles, for RPC scenarios, for m(LSP) = $0~\si {GeV}$ (dark shades) and $m(\text {mother}) - m(\text {LSP}) = 200~\si {GeV}$ (light shades); for each topology, for all results. In this plot, the lowest mass range is $m(\text {mother})=0$, but results are available starting from a certain mass depending on the analyses and topologies. Branching ratios of one are assumed, values shown in plot are to be interpreted as upper bounds on the mass limits \cite {cms_results}.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Best exclusion limits for the masses of the mother particles, for RPV scenarios, for each topology, for all results. In this plot, the lowest mass range is $m(\text  {mother})=0~\si {GeV}$, but results are available starting from a certain mass depending on the analyses and topologies. Branching ratios of one are assumed, values shown in plot are to be interpreted as upper bounds on the mass limits \cite  {cms_results}.\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:MODEL_DEPENDENT_2}{{1.3}{3}{Best exclusion limits for the masses of the mother particles, for RPV scenarios, for each topology, for all results. In this plot, the lowest mass range is $m(\text {mother})=0~\si {GeV}$, but results are available starting from a certain mass depending on the analyses and topologies. Branching ratios of one are assumed, values shown in plot are to be interpreted as upper bounds on the mass limits \cite {cms_results}.\relax }{figure.caption.4}{}}
\citation{baldi}
\citation{wulzer}
\citation{nielsen}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction to Neural Networks}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:NN}{{2}{5}{Introduction to Neural Networks}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Artificial Neural Networks}{5}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Mathematical model of a perceptron.\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:PERCEPTRON}{{2.1}{5}{Mathematical model of a perceptron.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Fully connected ANN.\relax }}{6}{figure.2.2}\protected@file@percent }
\newlabel{fig:FCNN}{{2.2}{6}{Fully connected ANN.\relax }{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Partially connected ANN.\relax }}{6}{figure.2.3}\protected@file@percent }
\newlabel{fig:NFCNN}{{2.3}{6}{Partially connected ANN.\relax }{figure.2.3}{}}
\citation{hanin}
\citation{oreilly}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Choice of NN architecture and activation functions}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Number of hidden layers}{7}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Number of neurons per hidden layer}{7}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Activation functions}{8}{subsection.2.2.3}\protected@file@percent }
\newlabel{fig:STEP}{{2.2.3}{8}{Activation functions}{subsection.2.2.3}{}}
\newlabel{fig:SIGMOID}{{2.2.3}{8}{Activation functions}{equation.2.2.5}{}}
\newlabel{fig:TANH}{{2.2.3}{8}{Activation functions}{equation.2.2.6}{}}
\newlabel{fig:RELU}{{2.2.3}{8}{Activation functions}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Loss functions}{8}{section.2.3}\protected@file@percent }
\citation{nielsen}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Backpropagation algorithm}{10}{section.2.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation through a typical deep neural network and the computation of the cost function. The loss $L(\mathaccentV {hat}05E{y},y)$ depends on the output $\mathaccentV {hat}05E{y}$ and on the target $y$. The symbol $\bm  {\theta }$ will be used to indicate both weights and biases.\relax }}{10}{algorithm.1}\protected@file@percent }
\newlabel{alg:FORWARD_PROPAGATION}{{1}{10}{Forward propagation through a typical deep neural network and the computation of the cost function. The loss $L(\hat {y},y)$ depends on the output $\hat {y}$ and on the target $y$. The symbol $\bm {\theta }$ will be used to indicate both weights and biases.\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backward computation for the deep neural network of Algorithm \ref  {alg:FORWARD_PROPAGATION}, which uses, in addition to the input $x$, a target $y$. This computation yields the gradients on the activations $a^{(k)}$ for each layer $k$, starting from the output layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer's output should change to reduce error, one can obtain the gradient on the parameters of each layer.\relax }}{11}{algorithm.2}\protected@file@percent }
\newlabel{alg:BACKWARD_COMPUTATION}{{2}{11}{Backward computation for the deep neural network of Algorithm \ref {alg:FORWARD_PROPAGATION}, which uses, in addition to the input $x$, a target $y$. This computation yields the gradients on the activations $a^{(k)}$ for each layer $k$, starting from the output layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer's output should change to reduce error, one can obtain the gradient on the parameters of each layer.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Updating free parameters: Optimizers}{11}{section.2.5}\protected@file@percent }
\newlabel{sec:OPTIMIZERS}{{2.5}{11}{Updating free parameters: Optimizers}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}An example of algorithm with fixed learning rate}{11}{subsection.2.5.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent (SGD) update at training iteration $k$.\relax }}{11}{algorithm.3}\protected@file@percent }
\newlabel{alg:SGD}{{3}{11}{Stochastic Gradient Descent (SGD) update at training iteration $k$.\relax }{algorithm.3}{}}
\citation{hinton}
\citation{srivastava}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}An example of algorithm with adaptive learning rates}{12}{subsection.2.5.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Adam algorithm.\relax }}{12}{algorithm.4}\protected@file@percent }
\newlabel{alg:ADAM}{{4}{12}{Adam algorithm.\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Other techniques}{12}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Regularizers}{12}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Dropout}{12}{subsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of dropout in NNs.\relax }}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:DROPOUT}{{2.4}{13}{Example of dropout in NNs.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Parameter initialization strategies}{13}{subsection.2.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Xavier and He initializations.\relax }}{13}{table.caption.7}\protected@file@percent }
\newlabel{tab:XAVIER_HE_INITIALIZATIONS}{{2.1}{13}{Xavier and He initializations.\relax }{table.caption.7}{}}
\citation{cern}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}CERN research for exotic particles through LHC collider}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:LHC}{{3}{15}{CERN research for exotic particles through LHC collider}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}An insight into LHC structure}{15}{section.3.1}\protected@file@percent }
\citation{cms}
\citation{cms}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematization of LHC collider.\relax }}{16}{figure.caption.8}\protected@file@percent }
\newlabel{fig:LHC_COLLIDER}{{3.1}{16}{Schematization of LHC collider.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}CMS experiment}{16}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces CMS detector section \cite  {cms}.\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{fig:CMS_SECTION}{{3.2}{17}{CMS detector section \cite {cms}.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces CMS detector transverse section.\relax }}{18}{figure.caption.10}\protected@file@percent }
\newlabel{fig:CMS_TRANSVERSE_SECTION}{{3.3}{18}{CMS detector transverse section.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}From data to discoveries: resonance formation study}{18}{section.3.3}\protected@file@percent }
\citation{bettini}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Resonance plot.\relax }}{19}{figure.caption.11}\protected@file@percent }
\newlabel{fig:RESONANCE_PLOT}{{3.4}{19}{Resonance plot.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Schematic of a resonance formation study in the cross section.\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:CROSS_SECTION_RESONANCE}{{3.5}{19}{Schematic of a resonance formation study in the cross section.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Schematic of a resonance formation study in the invariant mass.\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:INVARIANT_MASS_RESONANCE}{{3.6}{20}{Schematic of a resonance formation study in the invariant mass.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}$Z \rightarrow \mu ^{+}\mu ^{-}$ decaying process}{21}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Z}{{4}{21}{$Z \rightarrow \mu ^{+}\mu ^{-}$ decaying process}{chapter.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces $Z$ decay in the hypotesis of SM.\relax }}{21}{figure.4.1}\protected@file@percent }
\newlabel{fig:Z_MUMU_FEYNMAN}{{4.1}{21}{$Z$ decay in the hypotesis of SM.\relax }{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces $Z^{\prime }$ decay in the new physics scenario.\relax }}{21}{figure.4.2}\protected@file@percent }
\newlabel{fig:Z_PRIME_FEYNMAN}{{4.2}{21}{$Z^{\prime }$ decay in the new physics scenario.\relax }{figure.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The $Z^{0}$ boson}{21}{section.4.1}\protected@file@percent }
\citation{zboson}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{22}{section.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces High Level Features legend.\relax }}{22}{table.caption.14}\protected@file@percent }
\newlabel{tab:HLF}{{4.1}{22}{High Level Features legend.\relax }{table.caption.14}{}}
\newlabel{eqn:P_T_1}{{4.1}{22}{High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{equation.4.2.1}{}}
\newlabel{eqn:P_T_2}{{4.2}{22}{High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{equation.4.2.2}{}}
\newlabel{eqn:PR_1}{{4.3}{22}{High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{equation.4.2.3}{}}
\newlabel{eqn:PR_2}{{4.4}{22}{High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{equation.4.2.4}{}}
\newlabel{eqn:DELTA_PHI}{{4.5}{22}{High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{equation.4.2.5}{}}
\newlabel{eqn:INV_MASS}{{4.6}{22}{High Level Features for $\mu ^{+}\mu ^{-}$ decay analysis}{equation.4.2.6}{}}
\citation{madgraph}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The datasets analyzed}{23}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Features and invariant mass distributions for \textbf  {Zmumu-Zprime}.\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{fig:Z_PRIME_FEATURES_DISTRIBUTIONS}{{4.3}{24}{Features and invariant mass distributions for \textbf {Zmumu-Zprime}.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Features and invariant mass distributions for \textbf  {EFT\_YW06}.\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:EFT_YW06_FEATURES_DISTRIBUTIONS}{{4.4}{25}{Features and invariant mass distributions for \textbf {EFT\_YW06}.\relax }{figure.caption.16}{}}
\citation{wulzer}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Development of the algorithm for NP research}{27}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Z_5D}{{5}{27}{Development of the algorithm for NP research}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Statistical foundations}{27}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Construction of a test statistic}{27}{subsection.5.1.1}\protected@file@percent }
\newlabel{eqn:T}{{5.2}{27}{Construction of a test statistic}{equation.5.1.2}{}}
\citation{wulzer}
\citation{cowan}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Ideal test statistic}{28}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Adaptation of $t(\EuScript  {D})$ as a loss function}{28}{subsection.5.1.3}\protected@file@percent }
\newlabel{eqn:LOSS}{{5.10}{29}{Adaptation of $t(\mathcal {D})$ as a loss function}{equation.5.1.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}The algorithm}{29}{section.5.2}\protected@file@percent }
\citation{python}
\citation{keras}
\citation{tensorflow}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Computing resources}{30}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}\textsc  {TensorFlow} back-end and \textsc  {Keras} API}{30}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}LSF and clustering}{30}{subsection.5.3.2}\protected@file@percent }
\citation{cvmfs}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Available queues.\relax }}{31}{table.caption.17}\protected@file@percent }
\newlabel{tab:QUEUES}{{5.1}{31}{Available queues.\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Machines and resources.\relax }}{31}{table.caption.18}\protected@file@percent }
\newlabel{tab:LONG_QUEUE_MACHINES}{{5.2}{31}{Machines and resources.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Degrees of freedom of the network}{31}{section.5.4}\protected@file@percent }
\citation{look-elsewhere}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Neural Network scheme to understand the number of degrees of freedom.\relax }}{32}{figure.caption.19}\protected@file@percent }
\newlabel{fig:DOF}{{5.1}{32}{Neural Network scheme to understand the number of degrees of freedom.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The "look-elsewhere" effect}{32}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Weight clipping}{32}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Tuning of weight clipping parameter}{35}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:W_CLIP}{{6}{35}{Tuning of weight clipping parameter}{chapter.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Sizes of the datasets for different tests.\relax }}{35}{table.caption.20}\protected@file@percent }
\newlabel{tab:W_CLIP_TEST_SIZES}{{6.1}{35}{Sizes of the datasets for different tests.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}A criterion for optimal weight clipping}{35}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Results for optimal weight clipping search}{36}{section.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Results of optimal $W$ search for $W=2.15$.\relax }}{36}{figure.caption.21}\protected@file@percent }
\newlabel{fig:REF100000_BKG20000_SIG0_WCLIP2.15}{{6.1}{36}{Results of optimal $W$ search for $W=2.15$.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Results of optimal $W$ search for $W=2.4$.\relax }}{37}{figure.caption.22}\protected@file@percent }
\newlabel{fig:REF200000_BKG20000_SIG0_WCLIP2.4}{{6.2}{37}{Results of optimal $W$ search for $W=2.4$.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Results of optimal $W$ search for $W=2.3$.\relax }}{37}{figure.caption.23}\protected@file@percent }
\newlabel{fig:REF300000_BKG20000_SIG0_WCLIP2.3}{{6.3}{37}{Results of optimal $W$ search for $W=2.3$.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Results of optimal $W$ search for $W=2.55$.\relax }}{38}{figure.caption.24}\protected@file@percent }
\newlabel{fig:REF500000_BKG20000_SIG0_WCLIP2.55}{{6.4}{38}{Results of optimal $W$ search for $W=2.55$.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Training with signal events and results}{39}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:RESULTS}{{7}{39}{Training with signal events and results}{chapter.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusion and future developments}{41}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:CONCLUSION}{{8}{41}{Conclusion and future developments}{chapter.8}{}}
\bibcite{baldi}{1}
\bibcite{wulzer}{2}
\bibcite{cowan}{3}
\bibcite{cms_results}{4}
\bibcite{python}{5}
\bibcite{keras}{6}
\bibcite{tensorflow}{7}
\bibcite{cvmfs}{8}
\bibcite{lsf}{9}
\bibcite{madgraph}{10}
\bibcite{deeplearningbook}{11}
\bibcite{hanin}{12}
\bibcite{hinton}{13}
\bibcite{srivastava}{14}
\bibcite{oreilly}{15}
\bibcite{nielsen}{16}
\bibcite{cern}{17}
\bibcite{cms}{18}
\bibcite{bettini}{19}
\bibcite{zboson}{20}
\bibcite{look-elsewhere}{21}
